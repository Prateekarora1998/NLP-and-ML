{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding words into vector space\n",
    "\n",
    "Neural networks can only be applied to numeric valued inputs, since they rely on multiplication operations. This means if we want to apply a neural network to non-numeric data, such as text, it must first be converted into a numeric value. One very common way of doing this is to use one-hot encoding (also known as one-of-k encoding). In the one-hot encoding scheme, each input value is represented as a vector which has 0 in every component except for one component corresponding to this inputs unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to handle text input, every word in our language would be assigned a vector like this:\n",
    "\n",
    "$an = [1, 0, 0, \\dots, 0, 0]$\n",
    "\n",
    "$all = [0, 1, 0, \\dots, 0, 0]$\n",
    "\n",
    "$be = [0, 0, 1, \\dots, 0, 0]$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "$zebra = [0, 0, 0, \\dots, 1, 0]$\n",
    "\n",
    "$zoom = [0, 0, 0, \\dots, 0, 1]$\n",
    "\n",
    "Note that these vectors have a component for every word in our language, we will call the number of words in our language $|L|$, so these vectors have a shape of $[1, |L|]$. And note that $|L|$ could be a number in the tens of thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have converted each word into a vector, we can then use them as input to our neural network. The first hidden layer would apply a weight matrix of shape $[|L|, h_1]$, where $h_1$ is the number of neurons in the first hidden layer, which results in an output of shape $[1, h_1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's <a href=https://pytorch.org/docs/stable/nn.html#embedding>Embedding</a> module does both of these operations (one-hot encoding then matrix multiplication) in a computationally efficient way. Note that because all of our input vectors are of the special one-hot form, the result of the matrix multiplication is precisely equivalent to the $i$'th row of the matrix, where $i$ is the ID of the word. So all the embedding module does is lookup this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4199,  1.1700,  1.3843,  1.0550,  0.6570, -0.4245,  1.9782, -0.0851,\n",
       "          0.3551, -0.1129, -1.6164, -0.6281, -0.2600,  2.4676, -0.5192, -0.5041,\n",
       "         -0.1493, -1.3833,  1.4587,  0.4174],\n",
       "        [ 1.2303, -0.7818,  0.5052,  1.8135, -1.2525,  0.1615, -0.9466,  1.1362,\n",
       "          2.6283, -1.1375,  1.4876, -0.3981,  1.6128, -0.3662,  0.9357,  0.5452,\n",
       "          1.0750,  1.3526,  0.8984,  0.0572],\n",
       "        [ 0.2590, -1.6813,  1.6434, -0.4117,  0.0526,  0.5556, -0.8552,  0.9401,\n",
       "          0.7028,  1.1597, -0.7461, -1.1993,  2.0249, -0.5170, -1.8661, -0.3171,\n",
       "         -1.5135,  1.2348,  0.2522,  2.0199]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 5 # This is |L|.\n",
    "embedding_dim = 20 # This is h1.\n",
    "\n",
    "embedder = nn.Embedding(num_words, embedding_dim)\n",
    "\n",
    "input_words = torch.LongTensor([0, 3, 2]) # This is a batch of word IDs.\n",
    "\n",
    "out = embedder(input_words) # Apply the embedder to our input, the result is a batch of 3 vectors of size 20.\n",
    "out # These are known as the 'word embeddings'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
